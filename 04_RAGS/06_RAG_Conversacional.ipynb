{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the persistent directory\n",
    "current_dir = os.getcwd()\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_with_metadata\")\n",
    "\n",
    "# Define the embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever for querying the vector store\n",
    "# `search_type` specifies the type of search (e.g., similarity)\n",
    "# `search_kwargs` contains additional arguments for the search (e.g., number of results to return)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "# Create a ChatOpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextualizar la pregunta\n",
    "# Este prompt de sistema ayuda a la IA a entender que debe reformular la pregunta\n",
    "# en función del historial de chat para convertirla en una pregunta autónoma\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Dado un historial de chat y la última pregunta del usuario \"\n",
    "    \"que podría hacer referencia al contexto en el historial, \"\n",
    "    \"formula una pregunta autónoma que pueda entenderse \"\n",
    "    \"sin el historial de chat. NO respondas a la pregunta, solo \"\n",
    "    \"reformula si es necesario; si no, devuélvela tal cual.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for contextualizing questions\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a history-aware retriever\n",
    "# This uses the LLM to help reformulate the question based on chat history\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responder a la pregunta\n",
    "# Este prompt de sistema ayuda a la IA a entender que debe proporcionar respuestas concisas\n",
    "# basadas en el contexto recuperado e indica qué hacer si la respuesta es desconocida\n",
    "qa_system_prompt = (\n",
    "    \"Eres un asistente para tareas de preguntas y respuestas. Usa \"\n",
    "    \"las siguientes piezas de contexto recuperado para responder a la \"\n",
    "    \"pregunta. Si no sabes la respuesta, solo di que no la sabes. \"\n",
    "    \"Usa un máximo de tres oraciones y mantén la respuesta \"\n",
    "    \"concisa.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for answering questions\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain to combine documents for question answering\n",
    "# `create_stuff_documents_chain` feeds all retrieved context into the LLM\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the AI! Type 'exit' to end the conversation.\n",
      "AI: Homero es un poeta épico de la antigua Grecia, tradicionalmente considerado el autor de las obras clásicas \"La Ilíada\" y \"La Odisea\". Su existencia histórica es objeto de debate, y algunos estudiosos sugieren que pudo haber sido un individuo o una recopilación de fragmentos de varios poetas. Su obra ha tenido un profundo impacto en la literatura y la cultura occidental.\n",
      "AI: Algunos personajes importantes en \"La Odisea\" incluyen a Ulises (el protagonista y rey de Ítaca), Penélope (su fiel esposa), Telémaco (su hijo), Atenea (la diosa que lo ayuda), Poseidón (el dios del mar que le causa problemas), y Circe y Calypso (las diosas que lo retienen en sus islas). También son relevantes los pretendientes que asedian a Penélope en su ausencia y Eumeo, el porquero leal a Ulises. Estos personajes contribuyen a la exploración de temas como la lealtad, la astucia y el deseo de regresar a casa.\n",
      "AI: No tengo una lista específica de las mejores frases de \"La Odisea\" por personaje, pero algunos ejemplos representativos incluyen:\n",
      "\n",
      "- **Ulises**: Su ingenio y astucia se reflejan en sus diálogos sobre la estrategia en la guerra y su deseo de regresar a Ítaca.\n",
      "- **Penélope**: Expresa su lealtad y esperanza en momentos de desespero, mostrando su fortaleza ante la ausencia de Ulises.\n",
      "- **Atenea**: Suele ofrecer consejos sabios y motivadores, enfatizando la importancia de la inteligencia y la estrategia.\n",
      "- **Telémaco**: Sus palabras reflejan el crecimiento y la madurez a medida que busca a su padre y enfrenta a los pretendientes.\n",
      "\n",
      "Para citas específicas, se requeriría un análisis más detallado del texto.\n",
      "AI: No tengo acceso a una base de datos específica para realizar un análisis detallado de \"La Odisea\". Sin embargo, el libro se centra en el viaje de Ulises y sus desafíos para regresar a Ítaca tras la Guerra de Troya, explorando temas como la lealtad, la astucia, y el deseo de hogar. La obra también destaca la intervención de los dioses y las relaciones familiares, mostrando la lucha entre el destino y la libre voluntad.\n",
      "AI: No sé si tengo un retriever.\n",
      "AI: No tengo la capacidad de buscar en archivos o documentos específicos como \"odyssey.txt\". Sin embargo, puedo responder preguntas o proporcionar información general sobre \"La Odisea\". ¿Hay algo específico que te gustaría saber?\n",
      "AI: Lo más importante en \"La Odisea\" es el viaje de Ulises, que simboliza el deseo humano de regresar a casa y la lucha por la identidad y la pertenencia. A través de su odisea, se exploran temas como la astucia, la lealtad, y la intervención divina, así como las pruebas que enfrenta en su camino de regreso a Ítaca. La obra también destaca la importancia de las relaciones familiares, especialmente entre Ulises, Penélope y Telémaco.\n",
      "AI: You can learn more about LangChain by visiting its official documentation and GitHub repository, which provide comprehensive guides, tutorials, and examples. Additionally, exploring online courses, webinars, or community forums related to LangChain can enhance your understanding. Engaging with the community on platforms like Discord or Stack Overflow can also be helpful for real-time discussions and support.\n",
      "AI: Juliet died by taking her own life with a dagger. She believed that Romeo was dead and, in her grief, chose to end her life rather than live without him. This tragic decision occurred after she awoke from a potion-induced sleep to find Romeo had died by poison, thinking she was gone.\n",
      "AI: Juliet died by stabbing herself with a dagger after finding Romeo dead beside her. She had awakened from a potion-induced sleep to discover that Romeo had taken poison, believing she was dead, and in her despair, she chose to end her own life.\n"
     ]
    }
   ],
   "source": [
    "# Function to simulate a continual chat\n",
    "def continual_chat():\n",
    "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []  # Collect chat history here (a sequence of messages)\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        # Process the user's query through the retrieval chain\n",
    "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "        # Display the AI's response\n",
    "        print(f\"AI: {result['answer']}\")\n",
    "        # Update the chat history\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(SystemMessage(content=result[\"answer\"]))\n",
    "\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Curso_LC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
